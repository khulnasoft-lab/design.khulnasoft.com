---
name: AI-human interaction
related:
  - /usability/contextual-help
  - /usability/destructive-actions
  - /usability/saving-and-feedback
---

As Artificial Intelligence (AI) continues to advance, it brings both exciting opportunities and new challenges for product design. Although designing for AI still requires adhering to human-centered design principles, additional considerations such as ethics, privacy, trust, and transparency must be taken into account.

This page is divided into two main sections: [Guidelines](#guidelines) and [Framework](#framework) for AI.

## Guidelines

### Start with the user, not the technology

AI technology should be leveraged to enhance the user experience, rather than be the primary focus. Design with a deep understanding of the user's needs, goals, and pain points. If you aren't aligned with a user's need, you are building a system that does not solve a problem. Instead of asking "Can we use AI to _____?", ask yourself "How might we help users _____?".

### Understand when to automate

Understand if a task is a good fit for AI or if it is better done by a human. First, understand if a user's need will be helped by automation. Users may not want automation in high stakes tasks where they will be held responsible for the result, or tasks that they enjoy doing. Tasks that are a good fit for automation are tedious, error-prone, boring, low stakes, and free up the user's time. If a user benefits from automation, consider if the problem could be addressed with pre-defined rules (_if this, then that_). Understand the strengths and weaknesses of AI. AI can be helpful for processing large amounts of information, pattern finding, prediction, classification, and recommendations. Given good training data, AI can be more accurate and faster than a human at completing tasks. AI is less helpful for tasks requiring empathy, emotional intelligence, morality, common sense, predictability, contextual understanding, intuition, and creativity.

### Understand risk

Understand the risk of an AI-assisted feature by assessing the _probability_ and _impact_ of an incorrect recommendation. In a high stakes situation, the risk of negative consequences can be high. To mitigate risk in high stakes situations:

- Clarify the system's limitations and how much the user can trust its recommendations. For example, consider showing a detailed disclaimer such as “Content generated by AI should be seen as a starting point and verified before use. It may be incorrect, inappropriate, or diverge from your organization’s standards.” Or, if space is a concern, just “Verify before use.” See the related section, [Set the right expectations](#set-the-right-expectations).
- Design for potential negative impact. For example, a user should explicitly opt in to a high stakes AI-assisted feature.

### Communicate confidence

Users rely on the system to make decisions, but they should not trust the system entirely. Communicating confidence allows users to know how much scrutiny they should put recommendations under.

### Be transparent

Establish trust by ensuring the user always knows when they are interacting with AI, and when content or recommendations come from AI. Such disclosures are often required by third party AI services and may soon be required in the European Union ([EU AI Act](https://www.europarl.europa.eu/news/en/headlines/society/20230601STO93804/eu-ai-act-first-regulation-on-artificial-intelligence)).

#### Name

To communicate the suite of AI capabilities and identify specific AI-assisted features, use the [GitLab Duo](https://docs.google.com/presentation/d/1G849KWal8XDAEdusoR5YN8ZrZlvcgFVnqr4Nsjdy9Rc/edit#slide=id.g249996547b6_0_20) name. It's an extension of our brand that acts as an “umbrella” for all AI capabilities across GitLab. For variations of the GitLab Duo name, such as features or add-on, see the technical writing [word list](https://docs.gitlab.com/ee/development/documentation/styleguide/word_list.html).

- Show the “GitLab Duo” name at least once per AI-assisted feature. The name can be shown _before_ or _after_ user interaction.
- A call-to-action can optionally have the “GitLab Duo” name in its label, if reasonable. For example, “Ask GitLab Duo” or “Tell GitLab Duo what you're looking for…”

#### Disclaimer

- Flag AI-generated content with the passive voice disclaimer `<Verb> by AI`. For example: “Generated by AI” or “Summarized by AI”.
- Show the disclaimer only once per context, and preferably _under_ the AI-generated content, in a way that is clear that it applies to all content within that context.

#### Icon

In the UI, use the [`tanuki-ai`](https://gitlab-org.gitlab.io/gitlab-svgs/?q=~tanuki-ai) icon as the visual identifier for GitLab Duo.

- Show only one icon per context. For example, use only one instance of the icon in the header of a list or table, and not multiple instances for each child item.
- The icon is preferably shown _before_ the user interacts with the AI-assisted feature. For example, in the button that triggers the action. However, the icon can be shown _after_ user interaction, if more appropriate.

#### Illustration

The [`tanuki-ai-md`](https://gitlab-org.gitlab.io/gitlab-svgs/illustrations?q=illustrations/tanuki-ai-md.svg) and [`tanuki-ai-sm`](https://gitlab-org.gitlab.io/gitlab-svgs/illustrations?q=illustrations/tanuki-ai-sm.svg) illustrations serve the purpose of promoting and associating AI related features within the UI.

_The illustration is currently a [work in progress](https://gitlab.com/gitlab-org/gitlab-svgs/-/issues/400), and its final version is still in development._

#### Color

In the UI, there is _no specific color_ associated with AI or GitLab Duo — this differs from marketing, that has specific colors for the [GitLab Duo visual identity](https://docs.google.com/presentation/d/1G849KWal8XDAEdusoR5YN8ZrZlvcgFVnqr4Nsjdy9Rc/edit#slide=id.g252cac05ee9_0_17). The color of the icon or actions of AI-assisted features follow the component-specific guidelines, like [button](/components/button) variants.

### Set the right expectations

The interface should clearly communicate AI capabilities, limitations, and the scope of its decision-making authority. Users need to understand a system's capabilities and limits to understand how much trust to put into the system. To help the user build a mental model of the system:

- Clearly highlight if a feature is an [Experiment or Beta](/usability/feature-management#highlighting-feature-versions).
- Follow the [disclaimer guidelines](#disclaimer).
- Use clear, simple language to explain what the system is doing and how it arrived at its recommendations.
- Explain what data the system is trained on and what it's optimized for.
- Tell the user how their data is used and processed.

### Give the user control

The user should be able to decide whether to follow the AI's recommendations or not. There should be an easy way to undo system actions. Do not collect user data without asking the user's permission.

### Fail gracefully

When your system is not certain of the user's intent or has low confidence, make sure there is a path forward that does not rely on AI. Explain why the system was not able to provide a recommendation. Errors are also opportunities to learn more about your user's mental models and improve the system's ability to make recommendations. Consider designing a feedback mechanism that presents as a cue for adjustment rather than an error state.

### Encourage feedback

Design mechanisms to collect implicit and explicit feedback to improve the system.

## Framework

To help you put the [guidelines](#guidelines) into practice, the framework materializes them into standard patterns that address the most common UX challenges. Follow the progress in the [framework epic](https://gitlab.com/groups/gitlab-org/-/epics/10334).

### Dimensions

These dimensions can assist you in choosing the most appropriate pattern for the problem you are solving.

- **Mode**: What's the emphasis and persistence of the AI-human interaction relative to the main context and the user journey?
  - Focused: AI is the main context, with a dedicated focus.
  - Supportive: AI complements the main context and accompanies users along their journey to help them achieve their goals.
  - Integrated: AI is blended into specific moments of the users flow to help them complete small, discrete tasks.
- **Approach**: What should AI focus on improving?
  - Automate tasks: improve _efficiency_ by replacing human decision-making and actions, always done with human awareness and consent.
  - Augment capabilities: improve _effectiveness_ by supporting and improving human decision-making and actions.
- **Interactivity**: How does the system surface AI to engage with the user?
  - Proactive: triggered without user interaction.
  - Reactive: triggered by user interaction.
- **Task**: What's the user task that AI can assist with?
  - Classification: categorize, suggest, rank, match.
  - Generation: summarize, explain, create.
  - Prediction (or regression): forecast continuous, non-categorical data, like numerical values.

### Chat interaction for learning, ideation, and creation use cases

GitLab Duo Chat is a unified interface for users to interact with GitLab.

GitLab Duo Chat is also as a platform for internal teams and other contributors to contribute capabilities to.

We aim to employ the Chat for all use cases and workflows that can benefit from a **conversational** interaction **between** **a user** and **an AI** that is driven by a large language model (LLM). Typically, these are:

- **Creation and ideation** task that are more effectively and more efficiently solved through iteration than through a one-shot interaction.
- **Learning** tasks that are more effectively and more efficiently solved through iteration than through a one-shot interaction.
- **Tasks** that are typically satisfiable with one-shot interactions but **that might need refinement or could turn into a conversation**.
- Among the latter are tasks where the **AI may not get it right the first time but** where **users can easily course correct** by telling the AI more precisely what they need. For instance, "Explain this code" is a common question that most of the time would result in a satisfying answer, but sometimes the user will have additional questions.
- **Tasks that benefit from the history of a conversation**, so neither the user nor the AI need to repeat themselves.

The chat aims to be context aware and ultimately have access to all the resources in GitLab that the user has access to. Initially, this context is limited to code files, code selections, the content of individual issues and epics, as well as the GitLab documentation.

To scale the context awareness and the supported use cases across the entire DevSecOps domain, Duo Chat aims to be a platform, that other GitLab teams and the wider community can contribute to. They are the experts for the use cases and workflows to accelerate.

#### Interacting with Duo using natural language only

Users can formulate their own questions in the chat and the chat system will infer which context the user's question is refering to. E.g. it will know that when the user is looking at issue-123 and asks "What are the unique use cases raised in the comments of this issue?" that "this" refers to issue-123.

This only works if the chat system is aware of this resource. At the time of the writing of this text, Duo Chat was not aware of the diff in an MR for instance. Hence, a question "Can you explain the change this MR introduces?" is responded with "Unfortunately I do not have access to view the content of merge requests or make changes to repositories as a chatbot. I can only provide information based on issues, documentation, epics or CI configuration. If there is another way I can assist, please feel free to ask!"

So, if you want to contribute a use case in Duo Chat that requires context, you need to contribute a change that makes it available to the chat.

#### Initiating conversations about common tasks

Once the context is available (or if the use case does not require context data), you may want to contribute ways to iniate common tasks. At the time of writing the supported common tasks where to explain code, refactor code, and generate tests.

There are two UIs for iniating such common tasks:

- UI elements such as menu items or buttons
- slash commands

In GitLab, such UI elements have not been standardized, yet. In the IDE we have a context menu for that purpose.

Slash commands can be entered in the chat directly. The pattern is `/slash-command` + `(optional) additional instructions`. In the IDE, in some cases, it may be neccessary to have code selected for the slash command to make sense. Also, this part has not been standardized, yet, and need more thought.

#### Converation history

All interactions with the chat enter the conversation history, so the user and Duo can reference to it.

#### Validating and testing your use case

While we have accustomed to conversing with chats based on large language models we may not rely on the AI to do a perfect job.

Hence, before contributing to the chat you should validate that the LLM can solve your use case, if it has the needed context. Often this requires a collection of questions that users may typically ask in relation to your use case. You may consult the AI Model Validation Group if you need help.

After you have contributed your use case to the chat, you need to test whether the solution addresses the use case.

There are two ways of testing your use case solution:

- User tests: Collaborate with the UX research team to conduct such tests.
- Automatic tests: Use and contribute to the the Centrallized Evaluation Framework owned by the AI Model

Validation team to run tests as you develop your feature as wall as for monitoring your feature in production.

### Additional patterns

<todo>To add additional patterns patterns. Follow the progress in the [framework epic](https://gitlab.com/groups/gitlab-org/-/epics/10334).</todo>

While we don't have all patterns documented, we share some potential patterns in this [video](https://youtu.be/UXCz2xst_zg) ([slides](https://docs.google.com/presentation/d/1rO2BpI2WZC9Dxhv7oVR6XHk8GMb77AswESYcDANnQhA/edit?usp=sharing) and [internal Figma file](https://www.figma.com/file/s4TP1i2Akd1VTh4jhbg234/AI-prioritized-prototypes?type=design&node-id=2766-82606&t=zllXY21ifWzgeCq1-4)).

As inspiration for **integrated** mode patterns, you can find some explorations in [this Figma file](https://www.figma.com/file/s32hZcNQ0mQupGuEB5jUMH/Integrated-mode-AI-UX-patterns-design.gitlab.com%231615?type=design&node-id=1-2&mode=design):

<div class="figma-embed" aria-label="Examples of integrated mode patterns, showing how the AI-human interaction guidelines could be applied to a button, form, or static content." role="img">
  <iframe frameborder="0" src="https://www.figma.com/embed?embed_host=share&url=https%3A%2F%2Fwww.figma.com%2Ffile%2Fs32hZcNQ0mQupGuEB5jUMH%2FIntegrated-mode-AI-UX-patterns-design.gitlab.com%25231615%3Ftype%3Ddesign%26node-id%3D1%253A2%26mode%3Ddesign%26t%3DeWpvomQy7PhbCp4J-1" allowfullscreen></iframe>
</div>

## References

- [Conversational Experience Design, SAP](https://experience.sap.com/conversational-ux/)
- [Designing Intelligent Systems, Fiori for Web Design Guidelines, SAP](https://experience.sap.com/fiori-design-web/designing-intelligent-systems/)
- [Human-AI eXperience (HAX) Toolkit, Microsoft](https://www.microsoft.com/en-us/haxtoolkit/)
- [People + AI Guidebook, Google](https://pair.withgoogle.com/guidebook)
- [Responsible bots: 10 guidelines for developers of conversational AI, Microsoft](https://www.microsoft.com/en-us/research/uploads/prod/2018/11/Bot_Guidelines_Nov_2018.pdf)
